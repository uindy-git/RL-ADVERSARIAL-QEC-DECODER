# GNN-based QEC Decoders: Adversarial Probing and Robustness Enhancement

This repository contains the official source code and data to reproduce the results presented in our papers on developing, analyzing, and enhancing GNN-based decoders for Quantum Error Correction (QEC).

### Our Research Papers

1.  **[Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM](https://arxiv.org/abs/2508.03782)** *(arXiv:2508.03782)*

2.  **[Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning](https://arxiv.org/abs/2508.03783)** *(arXiv:2508.03783)*

---

## Abstract (from the RL-Adversarial paper)

Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approach for Quantum Error Correction (QEC) decoding, capable of learning complex noise characteristics directly from syndrome data. However, the robustness of these decoders against subtle, adversarial perturbations remains a critical open question. This work introduces a novel framework to systematically probe the vulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. The RL agent is trained as an adversary with the goal of finding minimal syndrome modifications that cause the decoder to misclassify. We apply this framework to a Graph Attention Network (GAT) decoder trained on experimental surface code data from Google Quantum AI. Our results show that the RL agent can successfully identify specific, critical vulnerabilities, achieving a high attack success rate with a minimal number of bit flips. Furthermore, we demonstrate that the decoder's robustness can be significantly enhanced through adversarial training, where the model is retrained on the adversarial examples generated by the RL agent. This iterative process of automated vulnerability discovery and targeted retraining presents a promising methodology for developing more reliable and robust neural network decoders for fault-tolerant quantum computing.

---

## Repository Structure

```
.
â”œâ”€â”€ ðŸ“œ README.md             # This file
â”œâ”€â”€ ðŸ“„ requirements.txt      # Required Python packages (excluding PyTorch/PyG)
â”‚
â”œâ”€â”€ ðŸ“‚ data/                  # Contains the preprocessed surface code data
â”‚   â””â”€â”€ ðŸ“‚ surface_code_bX_d3_r01_center_3_5/
â”‚       â”œâ”€â”€ ðŸ“„ circuit_detector_error_model.dem
â”‚       â”œâ”€â”€ ðŸ“„ detection_events_new.b8
â”‚       â””â”€â”€ ðŸ“„ obs_flips_actual_new.01
â”‚
â”œâ”€â”€ ðŸ“‚ src/                   # All Python source code
â”‚   â”œâ”€â”€ ðŸ“„ dataset.py         # Data loading and graph construction logic
â”‚   â”œâ”€â”€ ðŸ“„ models.py          # GAT decoder and RL Actor model definitions
â”‚   â”œâ”€â”€ ðŸ“„ train_decoder.py   # Script to train the baseline GAT decoder
â”‚   â”œâ”€â”€ ðŸ“„ train_rl_agent.py  # Script to train the adversarial RL agent
â”‚   â”œâ”€â”€ ðŸ“„ train_adversarial_decoder.py # Script for adversarial retraining
â”‚   â””â”€â”€ ðŸ“„ utils.py           # Helper functions (seeding, evaluation, plotting)
â”‚
â”œâ”€â”€ ðŸ“‚ models/                 # Directory where trained models are saved
â”‚
â”œâ”€â”€ ðŸ“‚ figures/               # Directory where generated figures are saved
â”‚
â””â”€â”€ ðŸ“„ main.py                 # Main entry point to run all experiments
```

---

## Installation

This project relies on specific versions of PyTorch and its ecosystem that must be compatible with your system's hardware (CPU or GPU/CUDA). Please follow these steps carefully to ensure a correct setup.

### 1. Create a Virtual Environment (Recommended)

First, create and activate a new Python virtual environment. This prevents conflicts with your other projects.

```bash
python -m venv venv
# On Windows:
# venv\Scripts\activate
# On macOS/Linux:
# source venv/bin/activate
```

### 2. Install PyTorch

The most critical step is to install the correct PyTorch version for your hardware. Visit the **[Official PyTorch Get Started Page](https://pytorch.org/get-started/locally/)** to find the precise command for your system.

* **For CPU-only systems:**
    The command will be similar to this:
    ```bash
    pip3 install torch torchvision torchaudio
    ```

* **For GPU systems (e.g., with CUDA 12.1):**
    The command will look something like this. **Do not copy this blindly!** Use the command generated on the PyTorch website for your specific CUDA version.
    ```bash
    # Example for CUDA 12.1
    pip3 install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
    ```

### 3. Install PyTorch Geometric

Once PyTorch is installed, you can install the compatible PyTorch Geometric libraries. The command depends on your PyTorch and CUDA version.

Find the correct command for your setup on the **[PyG Installation Guide](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html)**.

* **For CPU-only systems:**
    Find the command corresponding to your PyTorch version and `cpu`. For example:
    ```bash
    pip install torch_geometric
    pip install pyg_lib torch_scatter torch_sparse -f [https://data.pyg.org/whl/torch-2.3.0+cpu.html](https://data.pyg.org/whl/torch-2.3.0+cpu.html)
    ```

* **For GPU systems (e.g., with CUDA 12.1):**
    The command will look like this. Again, please verify on the PyG website.
    ```bash
    # Example for PyTorch 2.3 and CUDA 12.1
    pip install torch_geometric
    pip install pyg_lib torch_scatter torch_sparse -f [https://data.pyg.org/whl/torch-2.3.0+cu121.html](https://data.pyg.org/whl/torch-2.3.0+cu121.html)
    ```

### 4. Install Remaining Packages

Finally, install the rest of the dependencies from the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

Now your environment is ready to reproduce our research!

---

## How to Reproduce Results

The `main.py` script serves as the main entry point to run all experiments described in the papers. All models and figures will be saved to the `model/` and `figures/` directories respectively.

The entire experimental pipeline can be run with a single command:

```bash
python main.py
```

This will execute the following steps in sequence:
1.  Train the baseline GAT decoder (`train_decoder.py`).
2.  Train the RL agent against the baseline decoder (`train_rl_agent.py`).
3.  Evaluate the agent and generate the first vulnerability heatmap.
4.  Perform adversarial training to create a robust decoder (`train_adversarial_decoder.py`).
5.  Evaluate the original agent against the new robust decoder and generate the second heatmap.

---

## Citation

If you find this work useful in your research, please consider citing our papers:

```bibtex
@article{ikeda2025pi_gnn,
  title   = {Do {GNN}-based {QEC} Decoders Require Classical Knowledge? {E}valuating the Efficacy of Knowledge Distillation from {MWPM}},
  author  = {Ikeda, Ryota},
  journal = {arXiv preprint arXiv:2508.03782},
  year    = {2025}
}

@article{ikeda2025probing,
  title   = {Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning},
  author  = {Ikeda, Ryota},
  journal = {arXiv preprint arXiv:2508.03783},
  year    = {2025}
}
